{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ac53f-f1f8-412b-b172-8f7d8365308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "from TraceLens import TreePerfAnalyzer, GPUEventAnalyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a418fd8-014f-42b7-9411-a2d2ab046479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import schedule\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.bfloat16\n",
    "model  = models.resnet18().to(device).to(dtype)\n",
    "\n",
    "B, C, H, W   = 16, 3, 224, 224\n",
    "num_classes  = 1000\n",
    "\n",
    "dummy_input  = torch.randn(B, C, H, W,\n",
    "                           device=device, dtype=dtype)\n",
    "dummy_target = torch.randn(B, num_classes,\n",
    "                           device=device, dtype=dtype)\n",
    "\n",
    "def train_step():\n",
    "    # Single forward + backward pass.\n",
    "    output = model(dummy_input)\n",
    "    loss   = torch.nn.functional.mse_loss(output, dummy_target)\n",
    "    loss.backward()\n",
    "\n",
    "# test it out\n",
    "train_step()\n",
    "\n",
    "def warm_up(iters: int = 10):\n",
    "    for _ in range(iters):\n",
    "        train_step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\n",
    "sched_wait, sched_warmup, sched_active, sched_repeat = 10, 5, 3, 2\n",
    "sched = schedule(wait=sched_wait, \n",
    "                 warmup=sched_warmup, \n",
    "                 active=sched_active, \n",
    "                 repeat=sched_repeat)\n",
    "\n",
    "def trace_handler(p):\n",
    "    # this is called at the end of the active window\n",
    "    # ``p.step_num`` is the last iteration of the *active* window.\n",
    "    start = p.step_num - sched_active + 1\n",
    "    end   = p.step_num\n",
    "    p.export_chrome_trace(f\"trace_iter{start}_{end}.json\")\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU,\n",
    "                         ProfilerActivity.CUDA],\n",
    "             schedule=sched,\n",
    "             record_shapes=True,\n",
    "             with_stack=True,\n",
    "             on_trace_ready=trace_handler) as p:\n",
    "    warm_up()\n",
    "    for _ in range(100):\n",
    "        train_step()\n",
    "        p.step()                   # marks iteration boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d728b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the nn module flops analysis we need with_stack=True \n",
    "# and also include it for TraceLens by setting add_python_func=True \n",
    "perf_analyzer = TreePerfAnalyzer.from_file(\"/home/ajassani/iLT_playground/trace_iter16_18.json\", add_python_func=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80833caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility helpers for walking a TraceLens event-tree and estimating total FLOPs\n",
    "for a (sub)graph.  \n",
    "Key idea:  \n",
    "* If a CPU op has a registered performance model, we ask TraceLens for\n",
    "  its predicted FLOPs and stop recursion for that branch.  \n",
    "* If a CPU op is a *leaf* but **lacks** a model, we record it so the caller\n",
    "  can decide what to do with the unmodelled work.  \n",
    "* All other nodes (e.g. python functions, CPU ops with children, etc.)\n",
    "  are traversed recursively.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import TraceLens\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Logging configuration                                                      #\n",
    "# --------------------------------------------------------------------------- #\n",
    "# You can override this in the application’s entry-point if needed.\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,           # Change to INFO/ERROR in production.\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "\n",
    "# Module-level logger—importers can still reconfigure the root logger later.\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Mapping: op_name → PerfModel subclass (populated by TraceLens at import time)\n",
    "op_to_perf_model_class_map = TraceLens.PerfModel.op_to_perf_model_class_map\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Helper utilities                                                           #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def is_leaf_cpu_op(trace_tree, event) -> bool:\n",
    "    \"\"\"\n",
    "    Return ``True`` iff *event* is a CPU op **and** none of its children\n",
    "    are CPU ops that launch GPU work.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trace_tree : TraceTree\n",
    "        The TraceLens tree object that provides ``get_children_events``.\n",
    "    event : dict\n",
    "        A TraceLens event (host-side).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An op is not considered a leaf if any child has ``cat == \"cpu_op\"``\n",
    "    **and** also owns GPU events—those children do real compute work that\n",
    "    must be walked.\n",
    "    \"\"\"\n",
    "    for child in trace_tree.get_children_events(event):\n",
    "        if child.get(\"cat\") == \"cpu_op\" and child.get(\"gpu_events\") is not None:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Public API                                                                 #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def traverse_subtree_and_accumulate_flops(\n",
    "    trace_tree,\n",
    "    node: Dict[str, Any],\n",
    ") -> Tuple[int, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Compute total FLOPs rooted at *node* and gather CPU leaf ops\n",
    "    that lack a performance model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_flops : int\n",
    "        Sum of FLOPs for all nodes under *node* that we could model.\n",
    "    unaccounted_nodes : list[dict]\n",
    "        Leaf CPU ops with unknown cost (useful for modelling gaps).\n",
    "    \"\"\"\n",
    "    return _traverse_subtree_recursive(trace_tree, node)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "#  Internal recursive walker                                                  #\n",
    "# --------------------------------------------------------------------------- #\n",
    "def _traverse_subtree_recursive(\n",
    "    perf_analyzer,\n",
    "    node: Dict[str, Any],\n",
    ") -> Tuple[int, List[Dict[str, Any]]]:\n",
    "\n",
    "    trace_tree = perf_analyzer.tree\n",
    "\n",
    "    # 1️⃣  Skip nodes that never touch the GPU (pure bookkeeping / python work).\n",
    "    if node.get(\"gpu_events\") is None:\n",
    "        logger.debug(\"Skip non-GPU path  UID=%s  name=%s\",\n",
    "                     node.get(\"UID\"), node.get(\"name\"))\n",
    "        return 0, []\n",
    "\n",
    "    total_flops: int = 0\n",
    "    unaccounted_nodes: List[Dict[str, Any]] = []\n",
    "\n",
    "    cat = node.get(\"cat\")\n",
    "    logger.debug(\"Visit UID=%s  name=%s  cat=%s\",\n",
    "                 node.get(\"UID\"), node.get(\"name\"), cat)\n",
    "\n",
    "    # 2️⃣  CPU-op branch ------------------------------------------------------ #\n",
    "    if cat == \"cpu_op\":\n",
    "        name = node[\"name\"]\n",
    "\n",
    "        # 2.1  We *do* have a model → use it and stop here.\n",
    "        if name in op_to_perf_model_class_map:\n",
    "            perf_metrics = perf_analyzer.compute_perf_metrics(node)\n",
    "            flops_here = perf_metrics[\"GFLOPS\"] * 1e9\n",
    "            logger.debug(\"Model-hit  op=%s  FLOPs=%d\", name, flops_here)\n",
    "            return flops_here, unaccounted_nodes\n",
    "\n",
    "        # 2.2  Leaf CPU-op with **no** model → record it, stop here.\n",
    "        if is_leaf_cpu_op(trace_tree, node):\n",
    "            logger.debug(\"Unmodelled leaf CPU-op  UID=%s  name=%s\",\n",
    "                         node.get(\"UID\"), name)\n",
    "            unaccounted_nodes.append(node)\n",
    "            return 0, unaccounted_nodes\n",
    "\n",
    "    # 3️⃣  Generic branch (python funcs, or CPU-op with children) ------------ #\n",
    "    for child in trace_tree.get_children_events(node):\n",
    "        child_flops, child_unacc = _traverse_subtree_recursive(perf_analyzer, child)\n",
    "        total_flops += child_flops\n",
    "        unaccounted_nodes.extend(child_unacc)\n",
    "\n",
    "    logger.debug(\"Return UID=%s name=%s  cum_FLOPs=%d  unacc=%d\",\n",
    "                 node.get(\"UID\"), node.get(\"name\"),\n",
    "                 total_flops, len(unaccounted_nodes))\n",
    "    return total_flops, unaccounted_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee974fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the gpu event metrics for the nn.Module event\n",
    "nn_module_event = next(e for e in perf_analyzer.tree.events if e.get('name')=='nn.Module: BasicBlock_2')\n",
    "list_kernelUIDS = nn_module_event.get('gpu_events')\n",
    "list_kernels = [perf_analyzer.tree.events_by_uid[uid] for uid in list_kernelUIDS]\n",
    "gpu_time_metrics = GPUEventAnalyser(list_kernels).compute_metrics()\n",
    "pprint(gpu_time_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39daaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flops, unaccounted_nodes = traverse_subtree_and_accumulate_flops(perf_analyzer, nn_module_event)\n",
    "# unaccounted_nodes is a list of events that are cpu ops with no model\n",
    "print(f\"Total FLOPs: {total_flops}\")\n",
    "# print names of unaccounted nodes\n",
    "\n",
    "unaccounted_nodes_names = [node['name'] for node in unaccounted_nodes]\n",
    "print(\"Unaccounted nodes:\")\n",
    "for node in unaccounted_nodes:\n",
    "    print(node['UID'], node['name'])\n",
    "busy_tflops = (total_flops * 1e-12) / (gpu_time_metrics['busy_time'] * 1e-6)\n",
    "busy_tflops\n",
    "print(f\"Busy TFLOPS: {busy_tflops:.2f} TFLOPS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
