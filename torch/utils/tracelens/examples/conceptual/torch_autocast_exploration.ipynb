{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a7b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3219a2",
   "metadata": {},
   "source": [
    "By default we cannot multiply bf16 and fp32 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33adec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float\n"
     ]
    }
   ],
   "source": [
    "mat1 = torch.randn(2048, 1024, dtype=torch.bfloat16, device='cuda')\n",
    "mat2 = torch.randn(1024, 4096, dtype=torch.float32, device='cuda')\n",
    "\n",
    "try:\n",
    "    # Perform the matrix multiplication\n",
    "    result = torch.matmul(mat1, mat2)\n",
    "    # check dtype of the result\n",
    "    print(result.dtype)  # Should be torch.float32\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b9ddf",
   "metadata": {},
   "source": [
    "Within Autocast, same operation does not give error and gives bf16 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0cc805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying matrix multiplication for mat1: {mat1.dtype} and mat2: {mat2.dtype}\n",
      "Error during matrix multiplication: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float\n",
      "Performing matrix multiplication with autocast... for mat1: torch.bfloat16, mat2: torch.float32\n",
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "amp_ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n",
    "\n",
    "print(\"trying matrix multiplication for mat1: {mat1.dtype} and mat2: {mat2.dtype}\")\n",
    "try:\n",
    "    result = torch.matmul(mat1, mat2)\n",
    "except Exception as e:\n",
    "    print(f\"Error during matrix multiplication: {e}\")\n",
    "with amp_ctx:\n",
    "    print(f\"Performing matrix multiplication with autocast... for mat1: {mat1.dtype}, mat2: {mat2.dtype}\")\n",
    "    result_amp = torch.matmul(mat1, mat2)\n",
    "print(result_amp.dtype)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0862d459",
   "metadata": {},
   "source": [
    "Autocast policy varies by op type.<br> Layer Norm for example outputs the higher precision.<br>  See more detail here https://docs.pytorch.org/docs/stable/amp.html#cuda-op-specific-behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae326e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying layernorm with input data dtype torch.bfloat16 and weight dtype torch.float32\n",
      "Error during LayerNorm: expected scalar type BFloat16 but found Float\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Performing LayerNorm with autocast for input data dtype torch.bfloat16 and weight dtype torch.float32\n",
      "LayerNorm output with autocast dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# create a layernorm layer , init data and run it\n",
    "from torch import nn\n",
    "\n",
    "B, L, D = 32, 2048, 1024\n",
    "layernorm = nn.LayerNorm(D).to(device='cuda') # default dtype is float32\n",
    "\n",
    "input_data = torch.randn(B, L, D, dtype=torch.bfloat16, device='cuda')\n",
    "\n",
    "print(f\"trying layernorm with input data dtype {input_data.dtype} and weight dtype {layernorm.weight.dtype}\")\n",
    "try:\n",
    "    output_data = layernorm(input_data)\n",
    "    print(f\"LayerNorm output dtype: {output_data.dtype}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LayerNorm: {e}\")\n",
    "\n",
    "print('-' * 128)\n",
    "\n",
    "amp_ctx = torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16)\n",
    "print(f\"Performing LayerNorm with autocast for input data dtype {input_data.dtype} and weight dtype {layernorm.weight.dtype}\")\n",
    "with amp_ctx:\n",
    "    output_data_amp = layernorm(input_data)\n",
    "print(f\"LayerNorm output with autocast dtype: {output_data_amp.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766f6bf",
   "metadata": {},
   "source": [
    "Let's see what happens under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de79d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "def profile_fn(fn, path=\"fn_trace.json\", warmup=5, avg_steps=10):\n",
    "    \"\"\"\n",
    "    Profile fn\n",
    "    Args:\n",
    "        fn (callable): The function to profile.\n",
    "        warmup (int): Number of warmup iterations.\n",
    "        avg_steps (int): Number of iterations to average over.\n",
    "    Returns:\n",
    "        str: path of the replayed events trace\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def trace_handler(p):\n",
    "        p.export_chrome_trace(path)\n",
    "    activities = [ProfilerActivity.CPU, ProfilerActivity.CUDA]\n",
    "    wait = 10\n",
    "    warmup = 5\n",
    "    active = 10\n",
    "    with profile(\n",
    "        activities=activities,\n",
    "        schedule=torch.profiler.schedule(\n",
    "            wait=wait,\n",
    "            warmup=warmup,\n",
    "            active=active,\n",
    "            repeat=1\n",
    "            ),\n",
    "        record_shapes=True,\n",
    "        on_trace_ready=trace_handler\n",
    "    ) as p:\n",
    "        for idx in range(wait + warmup + active):\n",
    "            fn()\n",
    "            p.step()\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d19424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dtype: torch.bfloat16, shape: torch.Size([2048, 4096])\n",
      "Autocast profiling trace saved to matmul_autocast_trace.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W723 16:19:40.483997372 collection.cpp:1098] Warning: ROCTracer produced duplicate flow start: 26 (function operator())\n"
     ]
    }
   ],
   "source": [
    "mat1_add = torch.randn(2048, 1024, dtype=torch.bfloat16, device='cuda')\n",
    "mat2_add = torch.rand_like(mat1_add, dtype=torch.float32, device='cuda')\n",
    "def autocast_fn():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        result = torch.matmul(mat1, mat2)\n",
    "    return result\n",
    "\n",
    "result = autocast_fn()\n",
    "print(f\"Result dtype: {result.dtype}, shape: {result.shape}\")\n",
    "\n",
    "path = profile_fn(autocast_fn, path=\"matmul_autocast_trace.json\", warmup=5, avg_steps=10)\n",
    "print(f\"Autocast profiling trace saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29450f2a",
   "metadata": {},
   "source": [
    "We can see that the fp32 matrix is typecast into bf16 and then the op happens in low precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec43b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree with add_python_func=False\n",
      "Building CPU op tree with add_python_func=False\n",
      "└── UID: 1, Category: cpu_op, Name: aten::matmul\n",
      "    |   Input Dims: [[2048, 1024], [1024, 4096]]\n",
      "    |   Input type: ['c10::BFloat16', 'float']\n",
      "    |   \n",
      "    ├── UID: 2, Category: cpu_op, Name: aten::to\n",
      "    │   |   Input Dims: [[1024, 4096], [], [], [], []]\n",
      "    │   |   Input type: ['float', 'Scalar', 'Scalar', 'Scalar', '']\n",
      "    │   |   \n",
      "    │   └── UID: 3, Category: cpu_op, Name: aten::_to_copy\n",
      "    │       |   Input Dims: [[1024, 4096], [], [], [], [], [], []]\n",
      "    │       |   Input type: ['float', 'Scalar', '', '', '', 'Scalar', '']\n",
      "    │       |   \n",
      "    │       └── UID: 5, Category: cpu_op, Name: aten::copy_\n",
      "    │           |   Input Dims: [[1024, 4096], [1024, 4096], []]\n",
      "    │           |   Input type: ['c10::BFloat16', 'float', 'Scalar']\n",
      "    │           |   \n",
      "    │           └── UID: 90, Category: cuda_runtime, Name: hipLaunchKernel\n",
      "    │               └── UID: 210, Category: kernel, Name: void at::native::vectorized_elementwise_kernel<4, at::native::bf.., Duration: 23.199\n",
      "    └── UID: 6, Category: cpu_op, Name: aten::matmul\n",
      "        |   Input Dims: [[2048, 1024], [1024, 4096]]\n",
      "        |   Input type: ['c10::BFloat16', 'c10::BFloat16']\n",
      "        |   \n",
      "        └── UID: 7, Category: cpu_op, Name: aten::mm\n",
      "            |   Input Dims: [[2048, 1024], [1024, 4096]]\n",
      "            |   Input type: ['c10::BFloat16', 'c10::BFloat16']\n",
      "            |   \n",
      "            └── UID: 98, Category: cuda_runtime, Name: hipExtModuleLaunchKernel\n",
      "                └── UID: 212, Category: kernel, Name: Cijk_Ailk_Bljk_BBS_BH_Bias_HAS_SAV_UserArgs_MT256x64x32_MI32x32x.., Duration: 182.559\n"
     ]
    }
   ],
   "source": [
    "from TraceLens import TreePerfAnalyzer\n",
    "expt_perf_analyzer = TreePerfAnalyzer.from_file(path)\n",
    "evt = expt_perf_analyzer.tree.events[1]\n",
    "expt_perf_analyzer.tree.traverse_subtree_and_print(evt, cpu_op_fields=('Input Dims', 'Input type'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "347cf605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocast LayerNorm profiling trace saved to autocast_layernorm_trace.json\n"
     ]
    }
   ],
   "source": [
    "def autocast_layernorm_fn():\n",
    "    with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        output_data = layernorm(input_data)\n",
    "    return output_data\n",
    "\n",
    "path = profile_fn(autocast_layernorm_fn, path=\"autocast_layernorm_trace.json\", warmup=5, avg_steps=10)\n",
    "print(f\"Autocast LayerNorm profiling trace saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705a2cb",
   "metadata": {},
   "source": [
    "For Layer norm on the other hand, the low precision activation is upcasted and the operation happens in high precision. The cost of upcast is roughly same is the actual layer norm. Understanding autocast is important to understand performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6943193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tree with add_python_func=True\n",
      "Building CPU op tree with add_python_func=True\n",
      "└── UID: 1, Category: cpu_op, Name: aten::layer_norm\n",
      "    |   Input Dims: [[32, 2048, 1024], [], [1024], [1024], [], []]\n",
      "    |   Input type: ['c10::BFloat16', 'ScalarList', 'float', 'float', 'Scalar', 'Scalar']\n",
      "    |   \n",
      "    ├── UID: 2, Category: cpu_op, Name: aten::to\n",
      "    │   |   Input Dims: [[32, 2048, 1024], [], [], [], []]\n",
      "    │   |   Input type: ['c10::BFloat16', 'Scalar', 'Scalar', 'Scalar', '']\n",
      "    │   |   \n",
      "    │   └── UID: 3, Category: cpu_op, Name: aten::_to_copy\n",
      "    │       |   Input Dims: [[32, 2048, 1024], [], [], [], [], [], []]\n",
      "    │       |   Input type: ['c10::BFloat16', 'Scalar', '', '', '', 'Scalar', '']\n",
      "    │       |   \n",
      "    │       └── UID: 5, Category: cpu_op, Name: aten::copy_\n",
      "    │           |   Input Dims: [[32, 2048, 1024], [32, 2048, 1024], []]\n",
      "    │           |   Input type: ['float', 'c10::BFloat16', 'Scalar']\n",
      "    │           |   \n",
      "    │           └── UID: 130, Category: cuda_runtime, Name: hipLaunchKernel\n",
      "    │               └── UID: 190, Category: kernel, Name: void at::native::elementwise_kernel<512, 1, at::native::gpu_kern.., Duration: 474.72\n",
      "    └── UID: 6, Category: cpu_op, Name: aten::layer_norm\n",
      "        |   Input Dims: [[32, 2048, 1024], [], [1024], [1024], [], []]\n",
      "        |   Input type: ['float', 'ScalarList', 'float', 'float', 'Scalar', 'Scalar']\n",
      "        |   \n",
      "        └── UID: 7, Category: cpu_op, Name: aten::native_layer_norm\n",
      "            |   Input Dims: [[32, 2048, 1024], [], [1024], [1024], []]\n",
      "            |   Input type: ['float', 'ScalarList', 'float', 'float', 'Scalar']\n",
      "            |   \n",
      "            └── UID: 132, Category: cuda_runtime, Name: hipLaunchKernel\n",
      "                └── UID: 192, Category: kernel, Name: void at::native::(anonymous namespace)::vectorized_layer_norm_ke.., Duration: 473.119\n"
     ]
    }
   ],
   "source": [
    "from TraceLens import TreePerfAnalyzer\n",
    "path = \"autocast_layernorm_trace.json\"\n",
    "expt_perf_analyzer = TreePerfAnalyzer.from_file(path, add_python_func=True)\n",
    "evt = expt_perf_analyzer.tree.events[1]\n",
    "expt_perf_analyzer.tree.traverse_subtree_and_print(evt, cpu_op_fields=('Input Dims', 'Input type'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
