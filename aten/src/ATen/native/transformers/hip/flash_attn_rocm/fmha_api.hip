// !!! This is a file automatically generated by hipify!!!
// BSD 3 Clause
// Copyright 2023 Advanced Micro Devices, Inc.
// Redistribution and use in source and binary forms, with or without modification, are permitted provided that the
// following conditions are met:
// 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following
// disclaimer.
// 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the
// following disclaimer in the documentation and/or other materials provided with the distribution.
// 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote
// products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE
// COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
// COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
// OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR
// TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
// POSSIBILITY OF SUCH DAMAGE.

#include <ATen/ATen.h>
#include <c10/hip/HIPGuard.h>
// #include <ATen/native/transformers/hip/flash_attn_rocm/src/utils.h> // for unpack
// #include <ATen/hip/HIPGraphsUtils.cuh>                              // for philox
#ifndef __HIP_PLATFORM_AMD__
#define __HIP_PLATFORM_AMD__ 1
#endif
#include <hip/hip_runtime.h>
#include "src/incbin.h"

#include <iostream>

#define CHECK_SHAPE(x, ...) \
  TORCH_CHECK(x.sizes() == at::IntArrayRef({__VA_ARGS__}), #x " must have shape (" #__VA_ARGS__ ")")

#define FMHA_XSTR(s) FMHA_STR(s)
#define FMHA_STR(s) #s

#define FMHA_PRINT_NAME(name)                                               \
    do {                                                                    \
        std::cerr << __func__ << ":" << __LINE__ << " "                     \
                  << FMHA_STR(name) << " = " << name << std::endl;          \
    } while (0)

#define FMHA_PRINT_ATEN(name)                                               \
    do {                                                                    \
        FMHA_PRINT_NAME(name.size());                                       \
        for (size_t i = 0; i < name.size(); i++) {                          \
            std::cerr << "Vector of Tensors, Element " << i << std::endl;   \
            std::cerr << "Tensor Size " << name[i].sizes() << std::endl;    \
            std::cerr << name[i] << std::endl;                              \
        }                                                                   \
    } while (0);

#define HIP_CHECK_RETURN(expr)                                          \
    do {                                                                \
        auto r = (expr);                                                \
        if (r != hipSuccess)                                            \
            throw std::runtime_error("FAILURE at Line " C10_STRINGIZE(__LINE__) );   \
    } while(0)

INCBIN(fwd_kernel, "/home/xinyazha/ck-pytorch/aten/src/ATen/native/transformers/hip/flash_attn_rocm/src/amd_triton_kernel-4f89c7.hsaco", char);

namespace fmha {

template < class T >
std::ostream& operator << (std::ostream& os, const std::vector<T>& v) 
{
    os << "[ ";
    for (const auto& e : v)
    {
        os << " " << e;
    }
    os << " ]";
    return os;
}

class OortKernelArgs {
private:
  enum AType {
    POINTER,
    I32,
    F32
  };
  std::vector<void*> pointers_;
  std::vector<float> floats_;
  std::vector<int32_t> i32s_;
  std::vector<AType> atypes_;
  std::vector<void*> hip_args_;
public:
  void attach(void* p) {
    std::cerr << "pushing object: " << p << std::endl;
    pointers_.emplace_back(p);
    atypes_.emplace_back(POINTER);
  }
  void attach(float f) {
    std::cerr << "pushing float: " << f << std::endl;
    floats_.emplace_back(f);
    atypes_.emplace_back(F32);
  }
  void attach(int32_t i) {
    std::cerr << "pushing i32: " << i << std::endl;
    i32s_.emplace_back(i);
    atypes_.emplace_back(I32);
  }
  void attach_stride(const at::Tensor& tensor)
  {
    if (tensor.sizes().size() == 3) {
      attach(1);
      attach(static_cast<int32_t>(tensor.stride(0)));
      attach(static_cast<int32_t>(tensor.stride(1)));
      attach(static_cast<int32_t>(tensor.stride(2)));
    } else if (tensor.sizes().size() == 4) {
      attach(static_cast<int32_t>(tensor.stride(0)));
      attach(static_cast<int32_t>(tensor.stride(1)));
      attach(static_cast<int32_t>(tensor.stride(2)));
      attach(static_cast<int32_t>(tensor.stride(3)));
    } else {
      TORCH_CHECK(false, "len(Tensor sizes()) should be 3 or 4")
    }
#if 0
    for (auto i : indices) {
      attach(static_cast<int32_t>(tensor.stride(i)));
    }
#endif
  }
  void finalize() {
    hip_args_.clear();
    auto ptr_iter = pointers_.begin();
    auto i32_iter = floats_.begin();
    auto f32_iter = i32s_.begin();
    // FIXME: This is a prototype, we should handle things better
    for (auto t : atypes_) {
      switch (t) {
        case POINTER:
          hip_args_.emplace_back(&*ptr_iter);
          ptr_iter++;
          break;
        case I32:
          hip_args_.emplace_back(&*i32_iter);
          i32_iter++;
          break;
        case F32:
          hip_args_.emplace_back(&*f32_iter);
          f32_iter++;
          break;
      }
    }
  }
  void** get_hip_params() {
    return hip_args_.data();
  }
};

class OortKernel {
public:
  OortKernel(const char* kernel_name,
             const void* image)
  {
    hipJitOption opt[] = {hipJitOptionErrorLogBufferSizeBytes,
                          hipJitOptionErrorLogBuffer,
                          hipJitOptionInfoLogBufferSizeBytes,
                          hipJitOptionInfoLogBuffer, hipJitOptionLogVerbose};
    const unsigned int errbufsize = 8192;
    const unsigned int logbufsize = 8192;
    std::vector<char> err(errbufsize, 0);
    std::vector<char> log(errbufsize, 0);
    void *optval[] = {(void *)(uintptr_t)err.size(), err.data(),
                      (void *)(uintptr_t)log.size(), log.data(), (void *)(uintptr_t)1};

    HIP_CHECK_RETURN(hipModuleLoadDataEx(&mod_, image, 5, opt, optval));
    HIP_CHECK_RETURN(hipModuleGetFunction(&fun_, mod_, kernel_name));
  }
  void invoke(dim3 grid, dim3 block, int shared_memory_size, OortKernelArgs& args, hipStream_t stream)
  {
    HIP_CHECK_RETURN(hipModuleLaunchKernel(fun_,
                                           grid.x, grid.y, grid.z,
                                           block.x, block.y, block.z,
                                           shared_memory_size, stream, args.get_hip_params(), 0));
  }
  OortKernelArgs create_args() const {
    return OortKernelArgs();
  }
private:
    hipModule_t mod_;
    hipFunction_t fun_;
};

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor> mha_fwd(
    const at::Tensor& q,
    const at::Tensor& k,
    const at::Tensor& v,
    at::Tensor& out,
    const at::Tensor& cu_seqlens_q,
    const at::Tensor& cu_seqlens_k,
    const int max_seqlen_q,
    const int max_seqlen_k,
    const float p_dropout,
    const float softmax_scale,
    const bool zero_tensors,
    const bool is_causal,
    const bool return_softmax, // o in rocm ,this will return the random number matrix when doing dropout
    const int num_splits       // num_splits is not used in rocm
) {
  std::cerr << "Calling " << __func__ <<  std::endl;
  const bool is_deterministic = true;
  const bool is_using_qloop   = true;
  auto stream                 = at::cuda::getCurrentHIPStream().stream();
  bool is_dropout             = p_dropout > 0.0;
#if 0
  auto dprops                 = at::cuda::getCurrentDeviceProperties();
  LaunchParams<FlashFwdParams> launch_params(dprops, stream, is_dropout, return_softmax);
#endif

  auto q_dtype = q.dtype();

  TORCH_CHECK(q_dtype == at::kHalf || q_dtype == at::kBFloat16);
  TORCH_CHECK(k.dtype() == q_dtype);
  TORCH_CHECK(v.dtype() == q_dtype);
  TORCH_CHECK(out.dtype() == q_dtype);
  TORCH_CHECK(cu_seqlens_q.dtype() == at::kInt);
  TORCH_CHECK(cu_seqlens_k.dtype() == at::kInt);
  TORCH_CHECK(!is_dropout || return_softmax, "Prototype kernel doesn't support dropout (and consequently return_softmax)");

  TORCH_CHECK(q.is_cuda());
  TORCH_CHECK(k.is_cuda());
  TORCH_CHECK(v.is_cuda());
  TORCH_CHECK(out.is_cuda());

  TORCH_CHECK(q.stride(-1) == 1);
  TORCH_CHECK(k.stride(-1) == 1);
  TORCH_CHECK(v.stride(-1) == 1);
  TORCH_CHECK(out.stride(-1) == 1);
  TORCH_CHECK(cu_seqlens_q.is_contiguous());
  TORCH_CHECK(cu_seqlens_k.is_contiguous());

#if 1
  constexpr int TOTAL_DIM = 0;
  constexpr int H_DIM     = 1;
  constexpr int D_DIM     = 2;
  const auto sizes = q.sizes();

  const int batch_size = cu_seqlens_q.numel() - 1;
  const int total_q    = sizes[TOTAL_DIM];
  const int num_heads  = sizes[H_DIM];
  const int head_size  = sizes[D_DIM];
  const int total_k    = k.size(TOTAL_DIM);
  TORCH_CHECK(batch_size > 0);
  TORCH_CHECK((head_size % 8 == 0) && (head_size <= 128));

  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  CHECK_SHAPE(q, total_q, num_heads, head_size);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  CHECK_SHAPE(k, total_k, num_heads, head_size);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  CHECK_SHAPE(v, total_k, num_heads, head_size);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  CHECK_SHAPE(out, total_q, num_heads, head_size);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  CHECK_SHAPE(cu_seqlens_q, batch_size + 1);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  CHECK_SHAPE(cu_seqlens_k, batch_size + 1);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
#endif

  // Otherwise the kernel will be launched from cuda:0 device
  // Cast to char to avoid compiler warning about narrowing
  at::cuda::HIPGuard device_guard{(char)q.get_device()};

  auto opts = q.options();
  std::cerr << __func__ << ":" << __LINE__ << std::endl;

  auto softmax_lse = at::empty({batch_size, num_heads, max_seqlen_q}, opts.dtype(at::kFloat));
  // auto softmax_lse = at::full({batch_size, num_heads, max_seqlen_k}, -std::numeric_limits<float>::infinity(),
  // opts.dtype(at::kFloat));
  at::Tensor flash_softmax;
  if (return_softmax) {
    flash_softmax = at::empty({batch_size, num_heads, max_seqlen_q, max_seqlen_k}, opts.dtype(at::kInt));
  }
  if (zero_tensors) {
    out.zero_();
    // softmax_lse.zero_();
    softmax_lse.fill_(-std::numeric_limits<float>::infinity());
    if (return_softmax) {
      flash_softmax.zero_();
    }
  }
  std::cerr << __func__ << ":" << __LINE__ << std::endl;

#if 0
  auto gen =
      at::get_generator_or_default<at::CUDAGeneratorImpl>(c10::nullopt, at::cuda::detail::getDefaultCUDAGenerator());
#endif

  static OortKernel kernel("_fwd_kernel_0d1d2d34d5d6789101112131415161718192021222324",
                           fwd_kernel_start);

  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  /*
    Q, K, V, sm_scale,
    L,
    Out,
    stride_qz, stride_qh, stride_qm, stride_qk,
    stride_kz, stride_kh, stride_kn, stride_kk,
    stride_vz, stride_vh, stride_vk, stride_vn,
    stride_oz, stride_oh, stride_om, stride_on,
    Z, H, N_CTX,
    */
  auto args = kernel.create_args();
  // Q, K, V, sm_scale,
  args.attach(q.data_ptr());
  args.attach(k.data_ptr());
  args.attach(k.data_ptr());
  args.attach(softmax_scale);
  // L
  // L = torch.empty((q.shape[0] * q.shape[1], q.shape[2]), device=q.device, dtype=torch.float32)
  auto L = at::empty({q.size(0) * q.size(1),  q.size(2)},
                     at::TensorOptions().dtype(at::kHalf).device(at::kCUDA));
  args.attach(L.data_ptr());
  // Out
  args.attach(out.data_ptr());
  std::cerr << "q.sizes() = " << q.sizes() << std::endl;
  std::cerr << "k.sizes() = " << k.sizes() << std::endl;
  std::cerr << "v.sizes() = " << v.sizes() << std::endl;
  std::cerr << "out.sizes() = " << out.sizes() << std::endl;
#if 0
  const std::vector<int> stride_indices { 0, 1, 2, 3 };
  // strides
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(q, stride_indices);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(k, stride_indices);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(v, stride_indices);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(out, stride_indices);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
#else
  // strides
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(q);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(k);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(v);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
  args.attach_stride(out);
  std::cerr << __func__ << ":" << __LINE__ << std::endl;
#endif
  // Z, H, N_CTX,
  args.attach(static_cast<int32_t>(q.stride(0)));
  args.attach(static_cast<int32_t>(q.stride(1)));
  args.attach(static_cast<int32_t>(q.stride(2)));
  args.finalize();
  std::cerr << __FILE__ << ":" << __LINE__ << std::endl;

  constexpr int shared_memory_size = 16384; // Find this from json
  constexpr int num_warps = 8; // Find this from json
  constexpr int BLOCK_M = 64; // Find this from signature
  constexpr int BLOCK_DMODEL = 128; // Find this from signature
  constexpr int BLOCK_N = 64; // Find this from signature
  // python code: grid = (cdiv(q.shape[2], BLOCK_M), q.shape[0] * q.shape[1], 1)
  dim3 grid;
  grid.x = (q.size(2) + BLOCK_M - 1) / BLOCK_M;
  grid.y = q.size(0) * q.size(1);
  grid.z = 1;
  dim3 block { 64 * num_warps, 1, 1 };
  //
  kernel.invoke(grid, block, shared_memory_size,
                args, stream);

  at::Tensor seed_t   = at::empty({}, at::dtype(at::kLong));
  at::Tensor offset_t = at::empty({}, at::dtype(at::kLong));
  return {softmax_lse, seed_t, offset_t, flash_softmax};
#if 0 // Old code
  // number of times random will be generated per thread, to offset philox counter in thc random
  // state
  // We use a custom RNG that increases the offset by batch_size * nheads * 32.

  at::Tensor seed_t, offset_t;
  if (is_dropout) {
    // See Note [Acquire lock when using random generators]
    int64_t counter_offset           = launch_params.params.b * launch_params.params.h * 32;
    at::PhiloxCudaState philox_state = gen->philox_cuda_state(counter_offset);
    std::lock_guard<std::mutex> lock(gen->mutex_);
    if (at::cuda::currentStreamCaptureStatus() == at::cuda::CaptureStatus::None) {
      auto [seed, offset] = unpack(philox_state);
      seed_t              = at::scalar_tensor(at::Scalar(static_cast<int64_t>(seed)), at::dtype(at::kLong));
      offset_t            = at::scalar_tensor(at::Scalar(static_cast<int64_t>(offset)), at::dtype(at::kLong));
    } else {
      seed_t   = at::empty({}, at::dtype(at::kLong).device(at::kCUDA));
      offset_t = at::empty({}, at::dtype(at::kLong).device(at::kCUDA));
      // launch_params.params.seed = seed_t.data_ptr<int64_t>();
      // launch_params.params.extragraph_offset = offset_t.data_ptr<int64_t>();
    }
    launch_params.params.philox_args = philox_state;
  } else {
    if (at::cuda::currentStreamCaptureStatus() != at::cuda::CaptureStatus::None) {
      seed_t   = at::empty({}, at::dtype(at::kLong).device(at::kCUDA));
      offset_t = at::empty({}, at::dtype(at::kLong).device(at::kCUDA));
    } else {
      seed_t   = at::empty({}, at::dtype(at::kLong));
      offset_t = at::empty({}, at::dtype(at::kLong));
    }
  }

  run_flash_fwd(launch_params);

#if 0
    std::vector<at::Tensor> result = {softmax_lse};

    if (return_softmax) {
        result.push_back(s);
    }
    return result;
#endif
  return {softmax_lse, seed_t, offset_t, flash_softmax};
#endif
}

std::tuple<at::Tensor, at::Tensor, at::Tensor, at::Tensor> mha_bwd(
    const at::Tensor& dout,         // total_q x num_heads, x head_size
    const at::Tensor& q,            // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
    const at::Tensor& k,            // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
    const at::Tensor& v,            // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
    const at::Tensor& out,          // total_q x num_heads x head_size
    const at::Tensor& softmax_lse,  // b x h x s softmax logsumexp
    at::Tensor& dq,                 // total_q x num_heads x head_size, total_q := \sum_{i=0}^{b} s_i
    at::Tensor& dk,                 // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
    at::Tensor& dv,                 // total_k x num_heads x head_size, total_k := \sum_{i=0}^{b} s_i
    const at::Tensor& cu_seqlens_q, // b+1
    const at::Tensor& cu_seqlens_k, // b+1
    const int max_seqlen_q,
    const int max_seqlen_k,         // max sequence length to choose the kernel
    const float p_dropout,          // probability to drop
    const float softmax_scale,
    const bool zero_tensors,
    const bool is_causal,
    const int num_splits,
    at::Tensor philox_seed,  // Not used or supported on ROCm
    at::Tensor philox_offset // Not used or supported on ROCm
) {
  TORCH_CHECK(false, "Validation only, no backward flash attention support");
}

} // namespace fmha
